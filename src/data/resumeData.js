export const resumeData = {
  // Personal Information
  personal: {
    name: "Aanandhene M",
    title: "Data Engineer",
    location: "Erode, India",
    email: "aanandhene26@gmail.com",
    phone: "+91 6369675002",
    linkedin: "linkedin.com/in/aanandhene",
    github: "github.com/aanandhene",
    summary: "Proactive Data Engineer with 2+ years of experience building scalable, cloud-ready data platforms across streaming, batch, warehouse, and lakehouse systems. Skilled in designing pipelines using Kafka, Apache Flink, Python, SQL, and PostgreSQL. Strong focus on real-time analytics, data governance, and automation."
  },

  // Work Experience
  experience: [
    {
      id: 1,
      company: "Span Technology Services Private Ltd.",
      location: "Erode, India",
      position: "Junior Data Engineer",
      duration: "December 2023 - Present",
      type: "Full-time",
      description: "Product-based technology company delivering logistics, taxation, compliance, and enterprise data solutions for the US market",
      achievements: [
        "Managed end-to-end data engineering workflows across Sprint, Staging, UAT, and Live environments",
        "Designed and optimized streaming and batch ETL/ELT pipelines ensuring high data availability",
        "Built and supported data warehouse models with standardized schemas across business domains",
        "Implemented data governance, lineage, and quality checks using OpenMetadata",
        "Optimized SQL queries and improved system reliability during peak traffic seasons",
        "Enabled faster analytics and reporting through optimized data sources and dashboards"
      ],
      technologies: ["Python", "SQL Server", "PostgreSQL", "Apache Kafka", "Apache Flink", "Redis", "Airflow", "Docker", "OpenMetadata"]
    }
  ],

  // Education
  education: [
    {
      id: 1,
      institution: "Kongu Engineering College",
      location: "Tamil Nadu, India",
      degree: "Bachelor of Computer Science & Engineering",
      duration: "2020 - 2024",
      gpa: "8.83/10"
    }
  ],

  // Skills
  skills: {
    languages: [
      { name: "Python", level: 90 },
      { name: "SQL", level: 90 }
    ],
    frameworks: [
      { name: "Apache Kafka", level: 85 },
      { name: "Apache Flink", level: 85 },
      { name: "Apache Airflow", level: 80 }
    ],
    databases: [
      { name: "PostgreSQL", level: 85 },
      { name: "SQL Server", level: 80 },
      { name: "MongoDB", level: 75 },
      { name: "Redis", level: 75 },
      { name: "Supabase", level: 70 },
      { name: "Apache Doris", level: 80 }
    ],
    tools: [
      { name: "Docker", level: 80 },
      { name: "GitHub", level: 85 },
      { name: "Jenkins", level: 75 },
      { name: "Streamlit", level: 80 },
      { name: "Superset", level: 75 },
      { name: "Microsoft Power BI", level: 75 },
      { name: "OpenMetadata", level: 80 },
      { name: "Postman", level: 75 },
      { name: "n8n Automation", level: 70 }
    ]
  },

  // Projects
  projects: [
    {
      id: 1,
      title: "Enterprise Data Lakehouse Modernization",
      description: "Designed and built a scalable lakehouse architecture integrating relational and NoSQL sources into Apache Doris",
      role: "Data Engineer",
      duration: "2024",
      technologies: ["SQL Server", "MySQL", "MongoDB", "PostgreSQL", "Kafka", "Flink", "MinIO", "Airflow"],
      highlights: [
        "Improved analytical query performance by 70%",
        "Reduced API latency by 55%",
        "Implemented strong data governance with schema enforcement and lineage"
      ]
    },
    {
      id: 2,
      title: "Real-Time Payment Data Reconciliation",
      description: "Built real-time reconciliation pipelines for payment gateways",
      role: "Data Engineer",
      duration: "2024",
      technologies: ["Kafka", "Flink", "PostgreSQL", "Python", "Slack API"],
      highlights: [
        "Automated reconciliation and anomaly detection workflows",
        "Reduced manual checks by 85%",
        "Lowered duplicate payments by 60%"
      ]
    },
    {
      id: 3,
      title: "IRS Data Ingestion & Standardization Pipeline",
      description: "Automated ingestion and validation pipeline for IRS datasets",
      role: "Data Engineer",
      duration: "2024",
      technologies: ["Python", "PostgreSQL", "Airflow"],
      highlights: [
        "Improved data readiness by 3×",
        "Implemented validation, deduplication, and schema consistency checks"
      ]
    },
    {
      id: 4,
      title: "Real-Time Monitoring & Observability Framework",
      description: "Built monitoring and alerting framework for streaming jobs",
      role: "Data Engineer",
      duration: "2024",
      technologies: ["Python", "Flink", "Slack"],
      highlights: [
        "Increased incident detection by 80%",
        "Reduced recovery time by 50%"
      ]
    },
    {
      id: 5,
      title: "Streamlit-Based ETL Automation Studio",
      description: "Developed UI-driven ETL code generation platform",
      role: "Data Engineer",
      duration: "2024",
      technologies: ["Streamlit", "Python", "PyFlink"],
      highlights: [
        "Reduced ETL development time by 60%",
        "Standardized ingestion and transformation patterns"
      ]
    }
  ],

  // Certifications
  certifications: [
    "Microsoft Azure Fundamentals (AZ-900)",
    "Google Data Analytics – Coursera"
  ],

  // Awards
  awards: [
    "Span Super Star Award – Recognition for delivering critical projects under tight timelines and effective cross-team collaboration"
  ],

  // Social Links (Footer)
  socialLinks: {
    linkedin: "https://linkedin.com/in/aanandhene",
    github: "https://github.com/aanandhene"
  }
};
